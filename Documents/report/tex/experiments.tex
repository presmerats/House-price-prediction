\section{Experiments}

This section exposed the selected approach to perform a sound feature and model selection. We are usually faced with problems where feature selection is performed before model selection. In our case, with many different models that are so different between them, the selection of feature set can affect a lot the model performance. So in the end, to avoid performing space exploration without a strategy, we decide to choose between different strategical approaches:
\begin{itemize}
    \item Perform feature selection based on data analysis and then perform model selection over selected feature set
    \item pick a model based on tested assumptions on the data and then perform feature selection using forward selection while trainig the model
    \item perform PCA and select significant components, then perform model selection with the extracted PCA features and finally perform feature selection over all feature sets with the selected model.
    \item pick a baseline model type, and then perform feature selection while training the baseline model. Once the feature set is selected, we then could perform model selection over all the different types
    \item perform all the previous approaches and choose the pair of feature set and model that yields the lower validation error.
\end{itemize}

\subsection{Approach 1}

We concluded that this approach does not make sense, unless we limit our feature space to features extracted from a PCA analysis, where we can use the percentage of exaplined inertia to select features.

\input{./tables/ap1_results_04.tex}


\subsection{Approach 2}

This approach turns out to be exploration of the space of feature sets and models without any real guidance or strategy. Even if this if doable to certain extent, we decide to drop this approach as it won't be suitable in a real case with bigger data sets.


\input{./tables/ap2_results_04.tex}

\subsection{Approach 3}
In this experimental approach, we combine PCA for feature selection, then we perform model selection on that new dataset. Once the model has been selected, we perform again feature selection over all the feature sets (including again PCA)
 to finally select the features.\\ 
We begin by doing a Principal Components Analysis over all continuous variables of the dataset. Within this analysis we select the significant components (by the percentage of explained inertia) and then build a new preprocessed dataset using the projections of each individual on the principal components and its target value (the price). The next step is to train all different types of models over this dataset. For each model type, we perform cross validation to select the best hyper parameters of the model. We finally compare all the validation errors of the best models of each type to select our best model. Once the model is selected, we perform again crossvalidation over each of the feature data sets that we have availabe. In the same manner, the selected feature data set will be the one that yields the minimum validation error with the chosen model.

\input{./tables/ap3_results_04.tex}

\subsection{Approach 4}

In this experimental approach, we choose a baseline model and the train it over different feature sets in order to do feature selection. After that, all the models are trained over the selected feature set to do model selection.\\


\input{./tables/ap4_results_04.tex}

\subsection{Approach 5}

The last approach consists of gathering the results of previous approaches and compare. We can select the pair feature set and model that obtain the lower validation error.


\input{./tables/ap5_results_04.tex}

\subsection{Results}

% single option 1,2,3,4
The following table summarizes the results of the experiments performed. The models are trained over the selected feature set and then they are ranked according to their validation error. The model with minimal validation error will be our selection. 

%\input{./tables/experimentsResults.tex}

