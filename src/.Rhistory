#### 1 - Create contingency table ####
# Original contingency table
brun <- c(68,15,5,20, 108)
chatain <- c(119,54,29,84,286)
roux <- c(26,14,14,17,71)
blond <- c(7,10,16,94,127)
total <- c(220, 93,64,215,592)
m <- cbind(brun, chatain, roux, blond, total)
row.names(m) <- c("marron", "noisette", "vert", "bleu", "total")
# Visualization of points with first two factors.
m_points <- m[-nrow(m), -ncol(m)]
plot(m_points[,1:2], main = "Contingency table plot")
text(m_points[,1:2], row.names(m_points), pos=3)
##### 2 - Transformation of contingency table metrics. #####
# Get relative frequencies. Fi,j = Ki,j / N
m_freq <- m/m[nrow(m), ncol(m)]
# Get table assuming independence between factors.
fi <- m_freq[,5]
fj <- m_freq[5,]
m_independence <- m_freq
m_independence[,] <- 0
for (i in 1:5)
for (j in 1:5)
m_independence[i,j] <- fi[i] * fj[j]
# Get row profiles, that being the percentage of each category of var2 for each category of var 1.
row_profiles <- m_freq[,] / m_freq[,5]
row.names(row_profiles)[5] <- "Average profile"
row_profiles_100 <- row_profiles*100
# Get row profiles assuming independence.
row_profiles_independance <- m_independence[,] / m_independence[,5]
row.names(row_profiles_independance)[5] <- "Average profile"
row_profiles_independance_100 <- row_profiles_independance*100
row_profiles
plot(row_profiles[,1:2], main = "Contingency table plot")
text(row_profiles[,1:2], row.names(row_profiles), pos=3)
m_independence
m_freq
m_freq - m_independence
(m_freq - m_independence)^2
((m_freq - m_independence)^2)/m_independence
(0.047069028^2)0.06779584
(0.047069028^2)/0.06779584
X2_distances <- ((m_freq - m_independence)^2)/m_independence
X2_distances
X2_distances*100
Inertia <- sum(X2_distances)
Inertia
Inertia
projected_points <- eigen(cor(row_profiles[1:4,1:4]))
projected_points
sum(projected_points$values)
row_profiles[1:4,1:4]
cor(row_profiles[1:4,1:4])
m
Inertia*592
?CA
library(FactoMineR)
?CA
ca_result <- CA(m[1:4,1:4])
ca_result <- CA(m[1:4,1:4])
ca_result$eig
sum(ca_result$eig[,1])
row_profiles
?scale
scale_row_profiles <- scale(row_profiles[1:4,1:4])
projected_points <- eigen(cor(scale_row_profiles))
projected_points
sum(projected_points$values)
scale_row_profiles
row_profiles
row_profiles[1:4,1:4]
row_profiles
ca_result$eig
cor(scale_row_profiles)
cor(row_profiles[1:4,1:4])
ca_result$row$coord
plot(ca_result$row$coord[,1:2])
text (ca_result$row$coord, row.names(ca_result$row$coord))
abline(0,0,0,0)
ca_result$row$inertia
?dimdesc
rm(list=ls())
gc()
knitr::opts_chunk$set(echo = TRUE)
setwd("C:/Users/david/Documents/MIRI-Data Science/MVA - Multivariate Analysis/Homework/Week 6 - Advanced CA/")
library(FactoMineR)
cdg <- aggregate(as.data.frame(MCA_results$ind$coord[,1:6]), list(c1), mean)
rm(list=ls())
gc()
knitr::opts_chunk$set(echo = TRUE)
setwd("C:/Users/david/Documents/MIRI-Data Science/MVA - Multivariate Analysis/Homework/Week 6 - Advanced CA/")
library(FactoMineR)
cars_data <- read.csv("mca_car.csv")
summary(cars_data)
MCA_results <- MCA(cars_data, ncp=30, quanti.sup=c(17), quali.sup = c(18))
summary(MCA_results)
dimensions_inertia <- MCA_results$eig[,2]
individuals <- MCA_results$ind
variables <- MCA_results$var
plot(individuals$coord[,1:2], pch = 17, xlab = c(round(dimensions_inertia[1],2), " % of inertia"), ylab = c(round(dimensions_inertia[2],2), " % of inertia"))
#text(individuals$coord[,1:2], row.names(cars_data), cex=0.4)
abline(0,0,0, 0)
plot(variables$coord[,1:2], col="blue", pch=20)
text(variables$coord[,1:2], row.names(variables$coord), cex=0.7)
abline(0,0,0, 0)
plot(MCA_results$eig[,1], type="l")
# Significant dimensions:
mean_eighenvalue <- mean(MCA_results$eig[,1])
lmb <- MCA_results$eig[,1] - mean_eighenvalue
lmb <- lmb[lmb > 0]
plot(lmb/(sum(lmb)), type="l")
cumsum(100*lmb/sum(lmb))
nd <- 6
Psi <- MCA_results$ind$coord[,1:6]
d <- dist(Psi,method = "euclidean")
hc <- hclust(d, method = "ward.D2")
total_individuals <- nrow(cars_data)
barplot(hc$height[(total_individuals-40):(total_individuals-1)])
# we decide to keep 6 clusters.
nc <- 6
c1 <- cutree(hc, nc)
cdg <- aggregate(as.data.frame(MCA_results$ind$coord[,1:6]), list(c1), mean)
cdg
k_means_result <- kmeans(MCA_results$ind$coord[,1:6], centers = cdg)
cdg <- aggregate(as.data.frame(MCA_results$ind$coord[,1:6]), list(c1), mean)[,-1]
k_means_result <- kmeans(MCA_results$ind$coord[,1:6], centers = cdg)
k_means_result
plot(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], type = 'p', pch=17, col=k_means_result$cluster, main = "Clustering of cars in 6 classes")
text(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], labels = names(c1), pos = 4, cex=0.5)
x11()
plot(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], type = 'p', pch=17, col=k_means_result$cluster, main = "Clustering of cars in 6 classes")
text(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], labels = names(c1), pos = 4, cex=0.5)
plot(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], type = 'p', pch=17, col=k_means_result$cluster, main = "Clustering of cars in 6 classes")
abline(h=0, v=0, col='grey')
catdes(k_means_result)
?catdes
catdes(cbind(as.factor(k_means_result$cluster),cars_data),1)
rm(list=ls())
gc()
knitr::opts_chunk$set(echo = TRUE)
setwd("C:/Users/david/Documents/MIRI-Data Science/MVA - Multivariate Analysis/Homework/Week 6 - Advanced CA/")
library(FactoMineR)
cars_data <- read.csv("mca_car.csv")
summary(cars_data)
MCA_results <- MCA(cars_data, ncp=30, quanti.sup=c(17), quali.sup = c(18))
summary(MCA_results)
dimensions_inertia <- MCA_results$eig[,2]
individuals <- MCA_results$ind
variables <- MCA_results$var
plot(individuals$coord[,1:2], pch = 17, xlab = c(round(dimensions_inertia[1],2), " % of inertia"), ylab = c(round(dimensions_inertia[2],2), " % of inertia"))
#text(individuals$coord[,1:2], row.names(cars_data), cex=0.4)
abline(0,0,0, 0)
plot(variables$coord[,1:2], col="blue", pch=20)
text(variables$coord[,1:2], row.names(variables$coord), cex=0.7)
abline(0,0,0, 0)
plot(MCA_results$eig[,1], type="l")
# Significant dimensions:
mean_eighenvalue <- mean(MCA_results$eig[,1])
lmb <- MCA_results$eig[,1] - mean_eighenvalue
lmb <- lmb[lmb > 0]
plot(lmb/(sum(lmb)), type="l")
cumsum(100*lmb/sum(lmb))
nd <- 6
Psi <- MCA_results$ind$coord[,1:6]
d <- dist(Psi,method = "euclidean")
hc <- hclust(d, method = "ward.D2")
total_individuals <- nrow(cars_data)
barplot(hc$height[(total_individuals-40):(total_individuals-1)])
# we decide to keep 6 clusters.
nc <- 5
c1 <- cutree(hc, nc)
cdg <- aggregate(as.data.frame(MCA_results$ind$coord[,1:6]), list(c1), mean)[,-1]
k_means_result <- kmeans(MCA_results$ind$coord[,1:6], centers = cdg)
x11()
plot(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], type = 'p', pch=17, col=k_means_result$cluster, main = "Clustering of cars in 6 classes")
abline(h=0, v=0, col='grey')
plot(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], type = 'p', pch=17, col=k_means_result$cluster, main = "Clustering of cars in 6 classes")
abline(h=0, v=0, col='grey')
k_means_result
MCA_results <- MCA(cars_data, ncp=30, quanti.sup=c(17), quali.sup = c(18), plot=FALSE)
MCA_results <- MCA(cars_data, ncp=30, quanti.sup=c(17), quali.sup = c(18), graph =FALSE)
catdes(cbind(as.factor(k_means_result$cluster),cars_data),1)
summary(cars_data)
x11()
plot(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], type = 'p', pch=17, col=k_means_result$cluster, main = "Clustering of cars in 6 classes")
abline(h=0, v=0, col='grey')
legend(0,0,k_means_result$cluster)
legend(0,0,k_means_result$cluster, col=k_means_result)
legend(0,0,k_means_result$cluster, col=k_means_result$cluster)
x11()
plot(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], type = 'p', pch=17, col=k_means_result$cluster, main = "Clustering of cars in 6 classes")
abline(h=0, v=0, col='grey')
legend(0,0,k_means_result$cluster, col=k_means_result$cluster)
k_means_result$cluster
plot(MCA_results$ind$coord[1:10,1], MCA_results$ind$coord[1:10, 2], type = 'p', pch=17, col=k_means_result$cluster[1:10], main = "Clustering of cars in 6 classes")
abline(h=0, v=0, col='grey')
plot(MCA_results$ind$coord[1:10,1], MCA_results$ind$coord[1:10, 2], type = 'p', pch=17, col=k_means_result$cluster[1:10], main = "Clustering of cars in 6 classes")
abline(h=0, v=0, col='grey')
labels(MCA_results$ind$coord[1:20,1], MCA_results$ind$coord[1:20, 2], row.names(mca_cars)[1:20])
plot(MCA_results$ind$coord[1:20,1], MCA_results$ind$coord[1:20, 2], type = 'p', pch=17, col=k_means_result$cluster[1:10], main = "Clustering of cars in 6 classes")
labels(MCA_results$ind$coord[1:20,1], MCA_results$ind$coord[1:20, 2], row.names(mca_cars)[1:20])
abline(h=0, v=0, col='grey')
plot(MCA_results$ind$coord[1:20,1], MCA_results$ind$coord[1:20, 2], type = 'p', pch=17, col=k_means_result$cluster[1:10], main = "Clustering of cars in 6 classes")
labels(MCA_results$ind$coord[1:20,1], MCA_results$ind$coord[1:20, 2], row.names(mca_cars)[1:20])
abline(h=0, v=0, col='grey')
row.names(mca_cars)
plot(MCA_results$ind$coord[1:20,1], MCA_results$ind$coord[1:20, 2], type = 'p', pch=17, col=k_means_result$cluster[1:20], main = "Clustering of cars in 6 classes")
labels(MCA_results$ind$coord[1:20,1], MCA_results$ind$coord[1:20, 2], row.names(MCA_results$ind$coord[1:20,2]))
abline(h=0, v=0, col='grey')
plot(MCA_results$ind$coord[1:20,1], MCA_results$ind$coord[1:20, 2], type = 'p', pch=17, col=k_means_result$cluster[1:20], main = "Clustering of cars in 6 classes")
text(MCA_results$ind$coord[1:20,1], MCA_results$ind$coord[1:20, 2], row.names(MCA_results$ind$coord[1:20,2]))
abline(h=0, v=0, col='grey')
text(MCA_results$ind$coord[1:20,1], MCA_results$ind$coord[1:20, 2], row.names(MCA_results$ind$coord[1:20,2]))
x11()
plot(MCA_results$ind$coord[1:20,1], MCA_results$ind$coord[1:20, 2], type = 'p', pch=17, col=k_means_result$cluster[1:20], main = "Clustering of cars in 6 classes")
text(MCA_results$ind$coord[1:20,1], MCA_results$ind$coord[1:20, 2], row.names(MCA_results$ind$coord[1:20,2]))
abline(h=0, v=0, col='grey')
?text
plot(MCA_results$ind$coord[1:20,1], MCA_results$ind$coord[1:20, 2], type = 'p', pch=17, col=k_means_result$cluster[1:20], main = "Clustering of cars in 6 classes")
text(MCA_results$ind$coord[1:20,1], MCA_results$ind$coord[1:20, 2], labels = row.names(MCA_results$ind$coord[1:20,2]))
abline(h=0, v=0, col='grey')
x11()
plot(MCA_results$ind$coord[1:20,1], MCA_results$ind$coord[1:20, 2], type = 'p', pch=17, col=k_means_result$cluster[1:20], main = "Clustering of cars in 6 classes")
text(MCA_results$ind$coord[1:20,1], MCA_results$ind$coord[1:20, 2], labels = row.names(MCA_results$ind$coord[1:20,2]))
abline(h=0, v=0, col='grey')
row.names(MCA_results$ind$coord[1:20,2])
MCA_results$ind$coord[1:20,2]
names(MCA_results$ind$coord[1:20,2])
x11()
plot(MCA_results$ind$coord[1:20,1], MCA_results$ind$coord[1:20, 2], type = 'p', pch=17, col=k_means_result$cluster[1:20], main = "Clustering of cars in 6 classes")
text(MCA_results$ind$coord[1:20,1], MCA_results$ind$coord[1:20, 2], labels = names(MCA_results$ind$coord[1:20,2]))
abline(h=0, v=0, col='grey')
Psi <- MCA_results$ind$coord[,1:6]
d <- dist(Psi,method = "euclidean")
hc <- hclust(d, method = "ward.D2")
total_individuals <- nrow(cars_data)
barplot(hc$height[(total_individuals-40):(total_individuals-1)])
# we decide to keep 5 clusters.
nc <- 5
c1 <- cutree(hc, nc)
cdg <- aggregate(as.data.frame(MCA_results$ind$coord[,1:6]), list(c1), mean)[,-1]
k_means_result <- kmeans(MCA_results$ind$coord[,1:6], centers = cdg)
plot(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], type = 'p', pch=17, col=k_means_result$cluster, main = "Clustering of cars in 6 classes")
text(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], labels = names(MCA_results$ind$coord[,2]))
abline(h=0, v=0, col='grey')
Psi <- MCA_results$ind$coord[,1:6]
d <- dist(Psi,method = "euclidean")
hc <- hclust(d, method = "ward.D2")
total_individuals <- nrow(cars_data)
barplot(hc$height[(total_individuals-40):(total_individuals-1)])
# we decide to keep 5 clusters.
nc <- 5
c1 <- cutree(hc, nc)
cdg <- aggregate(as.data.frame(MCA_results$ind$coord[,1:6]), list(c1), mean)[,-1]
k_means_result <- kmeans(MCA_results$ind$coord[,1:6], centers = cdg)
plot(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], type = 'p', pch=17, col=k_means_result$cluster, main = "Clustering of cars in 6 classes")
text(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], labels = names(MCA_results$ind$coord[,2]))
abline(h=0, v=0, col='grey')
x11()
plot(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], type = 'p', pch=17, col=k_means_result$cluster, main = "Clustering of cars in 6 classes")
text(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], labels = names(c1), pos = 4, cex=0.5)
abline(h=0, v=0, col='grey')
k_means_result$cluster
order(k_means_result$cluster)
sort(k_means_result$cluster)
View(k_means_result$cluster)
View(cbind(k_means_result$cluster, names(k_means_result$cluster))
)
barplot(hc$height[(total_individuals-40):(total_individuals-1)])
barplot(hc$height[(total_individuals-40):(total_individuals-1)])
unique(names(c1))
plot(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], type = 'p', pch=17, col=k_means_result$cluster, main = "Clustering of cars in 6 classes")
text(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], labels = names(c1), pos = 4, cex=0.5)
abline(h=0, v=0, col='grey')
legend("topleft", legend=c("1", "", "3", "4", "5"), fill=c("black", "red", "green", "blue", "cyan"), cex=0.8)
plot(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], type = 'p', pch=17, col=k_means_result$cluster, main = "Clustering of cars in 6 classes")
abline(h=0, v=0, col='grey')
legend("topleft", legend=c("1", "2", "3", "4", "5"), fill=c("black", "red", "green", "blue", "cyan"), cex=0.8)
plot(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], type = 'p', pch=17, col=k_means_result$cluster, main = "Clustering of cars in 6 classes")
abline(h=0, v=0, col='grey')
legend("topleft", legend=c("1", "2", "3", "4", "5"), fill=c("black", "red", "green", "blue", "cyan"), cex=0.8)
?catdes
catdes(cbind(as.factor(k_means_result$cluster),cars_data))
catdes(cbind(as.factor(k_means_result$cluster),cars_data), 1, proba = 0.01)
summary(variables)
summary(cars_data)
length(k_means_result$cluster == 2)
table
table(k_means_result$cluster)
x11()
plot(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], type = 'p', pch=17, col=k_means_result$cluster, main = "Clustering of cars in 5 classes")
abline(h=0, v=0, col='grey')
legend("topleft", legend=c("High-End", "Diesel", "3", "Luxury", "5"), fill=c("black", "red", "green", "blue", "cyan"), cex=0.8)
?rm
rm(list = ls())
gc()
setwd("~/")
getwd()
david_ws <- "~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src"
setwd(david_ws)
getwd()
asaf_wd <- ""
pau_wd <- ""
?require
require("FactoMineR")
a = Sys.info()
a
a = Sys.info()
a[user]
a["user"]
if (a["user"] == "david") wd <- david_wd
david_wd <- "~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src"
asaf_wd <- ""
pau_wd <- ""
if (a["user"] == "david") wd <- david_wd
wd
if (a["user"] == "david")
{
wd <- david_wd
}
elseif (a["user"] == "pau")
{
wd <- pau_wd
} else {
wd <- asaf_wd
}
if (a["user"] == "david")
{
wd <- david_wd
} else if (a["user"] == "pau")
{
wd <- pau_wd
} else {
wd <- asaf_wd
}
setwd(wd)
source("aux_functions.R")
load_packages()
load_packages
load_packages()
if (a[1] == "Linux"){
# Linux reading file
data <- read.csv(file="../Dataset/kc_house_data.csv", header=TRUE, sep=",")
} else {
# windows reading file
data <- read.csv(file="../Dataset/kc_house_data.csv", header=TRUE, sep=",")
}
mvn(data=data, mvnTest="mardia")
require("mvtnorm")
mvn(data=data, mvnTest="mardia")
?mvn
??mvn
require("MVN")
mvn(data=data, mvnTest="mardia")
summary(data)
str(data)
data <- data[,-c("id", "date")]
data <- data[,-c("id", "date")]
data <- data[,-("id", "date")]
data <- data[,-c("id")]
data <- data[,-c("id")]
data <- data[,-("id")]
categorical_vars <- c("id", "date", "waterfront", "view", "yr_built", "yr_renovated", "zipcode", "lat", "long")
index_cat_vars <- names(data) %in% categorical_vars
index_cat_vars
data <- data[,!removed_vars]
removed_vars <- names(data) %in% categorical_vars
data <- data[,!removed_vars]
data
names(data)
# test multivariate normality
mvn(data=data, mvnTest="mardia")
?qqplot
?saphiro.test
??saphiro.test
??ssapiro.test
??shapiro.test
source_scripts()
source("aux_functions.R")
source_scripts()
require("mvnormtest")
# test multivariate normality
mshapiro.test(data[1:5000,])
str(data)
# test multivariate normality
mshapiro.test(data)
# test multivariate normality
mshapiro.test(as.matrix(data))
# test multivariate normality
mshapiro.test(as.matrix(data[1:5000,]))
rca_result <- PCA(data)
?pca
?PCA
rca_result <- PCA(data, quati.sup = "price")
names(data)
rca_result <- PCA(data, quati.sup = 1)
rca_result <- PCA(data, quanti.sup = "price")
rca_result <- PCA(data, quanti.sup = c(1))
p <- recordPlot()
x11()
p
?jpeg
jpeg(filename = "test.jpeg")
pca_result <- PCA(data, quanti.sup = c(1))
dev.off()
filename <- paste(output_results, "PCA_variables_projection_first_factorial_plane.jpeg", sep = "")
output_results = "../Analysis Results/Unsupervised Analysis/"
filename <- paste(output_results, "PCA_variables_projection_first_factorial_plane.jpeg", sep = "")
filename
jpeg(filename = filename)
pca_result <- PCA(data, quanti.sup = c(1))
dev.off()
jpeg(filename = filename, width = 1000, height=1000)
pca_result <- PCA(data, quanti.sup = c(1))
dev.off()
?writeLines
filename <- paste(output_results, "results.txt", sep = "")
fileConn<-file(filename)
writeLines(c("Hello","World"), fileConn)
close(fileConn)
close(fileConn)
fileConn<-file(filename)
result <- c("PCA_analysis result: several highly correlated variables, explaining the price. Recommended to identify latent factors and use them for prediction")
writeLines(result, fileConn)
close(fileConn)
source("linear_regressions_fitting.R")
names(Data)
names(data)
lm_model <- lm(price ~ ., data=data)
summary(lm_model)
lmtest::dwtest(lm_model, alternative="two.sided") # Independance among all the samples - Durbin Watson Test.
shapiro.test(residuals(lm_model)) # All populations are normal - Saphiro Test
lmtest::bgtest(lm_model) # Equal variances - Homogeneity of variances.
plot(residuals)
residuals
plot(residuals(lm_model))
plot(residuals(lm_model)[1:5000])
x11()
plot(residuals(lm_model)[1:5000])
shapiro.test(residuals(lm_model)[1:4990]) # All populations are normal - Saphiro Test
RSE <- price - residuals(lm_model)
RSE <- data$price - residuals(lm_model)
RSE <- sqrt((1/nrow(data)*sum(residuals(lm_model))))
RSE
?residuals
plot(data$price)
summary(lm_model)
sum(residuals(lm_model))
sum(abs(residuals(lm_model)))
3404994885/21612
sum(residuals(lm_model)^2)
sum(residuals(lm_model)^2)/21613
sqrt(sum(residuals(lm_model)^2)/21613)
sqrt(sum(residuals(lm_model)^2)/21611)
sqrt(sum(residuals(lm_model)^2)/21613)
RSE <- sqrt(sum(residuals(lm_model)^2)/21613)
RSE <- sqrt(sum(residuals(lm_model)^2)/nrow(data)) # average error made on a prediction.
RSE
lm_model$terms
lm_model$qr
lm_model$df.residual
lm_model$residuals
lm_model$xlevels
lm_model$terms
summary(lm_model)
R_sqared <- 0.56
mean_price <- mean(data$price)
TSS <- sum(data$price - mean_price)
TSS
TSS <- sum((data$price - mean_price)^2)
TSS
RSS <- sum(residuals(lm_model)^2)
RSE <- sqrt(RSS/nrow(data)) # average error made on a prediction.
TSS <- sum((data$price - mean_price)^2)
R_sqared <- (TSS - RSS)/RSS
R_sqared <- (TSS - RSS)/TSS
R_sqared
result <- c("lm_fitting_continuous_original_variables_result: R-squared:", R_sqared)
result
result <- c("lm_fitting_continuous_original_variables_result: R-squared:", round(R_sqared),2)
result
result <- c("lm_fitting_continuous_original_variables_result: R-squared:", round(R_sqared, 2))
result
output_results = "../Analysis Results/Linear Model Fitting"
filename <- paste(output_results, "results.txt", sep = "")
filename <- paste(output_results, "Results.txt", sep = "")
filename
output_results = "../Analysis Results/Linear Model Fitting/
""
"
output_results = "../Analysis Results/Linear Model Fitting/"
filename <- paste(output_results, "Results.txt", sep = "")
filename
fileConn<-file(filename)
result <- c("lm_fitting_continuous_original_variables_result: R-squared:", round(R_sqared, 2))
writeLines(result, fileConn)
close(fileConn)
result
result <- c("lm_fitting_continuous_original_variables_result: R-squared:", round(R_sqared, 2), sep="")
result
result <- paste("lm_fitting_continuous_original_variables_result: R-squared:", round(R_sqared, 2), sep="")
result
fileConn<-file(filename)
result <- paste("lm_fitting_continuous_original_variables_result: R-squared:", round(R_sqared, 2), sep="")
writeLines(result, fileConn)
close(fileConn)
fileConn<-file(filename)
result <- paste("lm_fitting_continuous_original_variables_result: R-squared:", round(R_sqared, 2), sep="")
writeLines(result, fileConn)
writeLines(summary(lm_model), fileConn)
close(fileConn)
summary(lm_model)
