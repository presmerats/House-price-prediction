Psi <- MCA_results$ind$coord[,1:6]
d <- dist(Psi,method = "euclidean")
hc <- hclust(d, method = "ward.D2")
total_individuals <- nrow(cars_data)
barplot(hc$height[(total_individuals-40):(total_individuals-1)])
# we decide to keep 5 clusters.
nc <- 5
c1 <- cutree(hc, nc)
cdg <- aggregate(as.data.frame(MCA_results$ind$coord[,1:6]), list(c1), mean)[,-1]
k_means_result <- kmeans(MCA_results$ind$coord[,1:6], centers = cdg)
plot(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], type = 'p', pch=17, col=k_means_result$cluster, main = "Clustering of cars in 6 classes")
text(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], labels = names(MCA_results$ind$coord[,2]))
abline(h=0, v=0, col='grey')
Psi <- MCA_results$ind$coord[,1:6]
d <- dist(Psi,method = "euclidean")
hc <- hclust(d, method = "ward.D2")
total_individuals <- nrow(cars_data)
barplot(hc$height[(total_individuals-40):(total_individuals-1)])
# we decide to keep 5 clusters.
nc <- 5
c1 <- cutree(hc, nc)
cdg <- aggregate(as.data.frame(MCA_results$ind$coord[,1:6]), list(c1), mean)[,-1]
k_means_result <- kmeans(MCA_results$ind$coord[,1:6], centers = cdg)
plot(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], type = 'p', pch=17, col=k_means_result$cluster, main = "Clustering of cars in 6 classes")
text(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], labels = names(MCA_results$ind$coord[,2]))
abline(h=0, v=0, col='grey')
x11()
plot(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], type = 'p', pch=17, col=k_means_result$cluster, main = "Clustering of cars in 6 classes")
text(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], labels = names(c1), pos = 4, cex=0.5)
abline(h=0, v=0, col='grey')
k_means_result$cluster
order(k_means_result$cluster)
sort(k_means_result$cluster)
View(k_means_result$cluster)
View(cbind(k_means_result$cluster, names(k_means_result$cluster))
)
barplot(hc$height[(total_individuals-40):(total_individuals-1)])
barplot(hc$height[(total_individuals-40):(total_individuals-1)])
unique(names(c1))
plot(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], type = 'p', pch=17, col=k_means_result$cluster, main = "Clustering of cars in 6 classes")
text(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], labels = names(c1), pos = 4, cex=0.5)
abline(h=0, v=0, col='grey')
legend("topleft", legend=c("1", "", "3", "4", "5"), fill=c("black", "red", "green", "blue", "cyan"), cex=0.8)
plot(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], type = 'p', pch=17, col=k_means_result$cluster, main = "Clustering of cars in 6 classes")
abline(h=0, v=0, col='grey')
legend("topleft", legend=c("1", "2", "3", "4", "5"), fill=c("black", "red", "green", "blue", "cyan"), cex=0.8)
plot(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], type = 'p', pch=17, col=k_means_result$cluster, main = "Clustering of cars in 6 classes")
abline(h=0, v=0, col='grey')
legend("topleft", legend=c("1", "2", "3", "4", "5"), fill=c("black", "red", "green", "blue", "cyan"), cex=0.8)
?catdes
catdes(cbind(as.factor(k_means_result$cluster),cars_data))
catdes(cbind(as.factor(k_means_result$cluster),cars_data), 1, proba = 0.01)
summary(variables)
summary(cars_data)
length(k_means_result$cluster == 2)
table
table(k_means_result$cluster)
x11()
plot(MCA_results$ind$coord[,1], MCA_results$ind$coord[, 2], type = 'p', pch=17, col=k_means_result$cluster, main = "Clustering of cars in 5 classes")
abline(h=0, v=0, col='grey')
legend("topleft", legend=c("High-End", "Diesel", "3", "Luxury", "5"), fill=c("black", "red", "green", "blue", "cyan"), cex=0.8)
rm(list = ls())
gc()
#### 1 - Set up environment ####
a = Sys.info()
david_wd <- "~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src"
asaf_wd <- ""
pau_wd <- ""
if (a["user"] == "david") {
wd <- david_wd
} else if (a["user"] == "pau") {
wd <- pau_wd
} else {
wd <- asaf_wd
}
setwd(wd)
# source scripts
source("aux_functions.R")
source_scripts()
load_packages()
#### 2 - Read data ####
if (a[1] == "Linux"){
# Linux reading file
data <- read.csv(file="../Dataset/kc_house_data.csv", header=TRUE, sep=",")
} else {
# windows reading file
data <- read.csv(file="../Dataset/kc_house_data.csv", header=TRUE, sep=",")
}
library(MASS)
## first we fix the seed so we all get the same results
set.seed(1234567)
## We are going to design an artificial 2-class problem, where each class is a multivariate gaussian
## Let us first generate a symmetric positive-definite (PD) matrix that will
## constitute the covariance matrix of our Gaussians
## We do this by sampling from a Wishart distribution
## for more details, see rWishart {stats}
S <- toeplitz((2:1)/2)
# this S matrix is our reference matrix (the parameter of the Wishart) and is PD
S
## now we sample from the Wishart distribution
R <- rWishart(1, 4, S)[,,1]
## we get a feasible random 2x2 covariance matrix
dim(R)
R
## now we generate the data for the 2 classes; both classes will share the same
## covariance matrix R that we just generated
## we set 2/3 and 1/3 for the two class priors and (-1,0), (1,1/2) for the two means
## total desired size of the data set (both classes together)
N <- 200
## set class sizes according to priors
prior.1 <- 2/3
prior.2 <- 1/3
N1 <- round(prior.1*N)
N2 <- N - N1
## sample from the gaussians
data1 <- mvrnorm(N1, mu=c(-1,0), Sigma=R)
data2 <- mvrnorm(N2, mu=c(1,1/2), Sigma=R)
## Now we create a dataframe with all data stacked by rows, plus the target (the class)
data <- data.frame (rbind(data1,data2),target=as.factor(c(rep('1',N1),rep('2',N2))))
summary(data)
## these are the two classes
plot(data$X1,data$X2,col=data$target)
R
my.lda <- lda(target ~ X1 + X2, data = data)
(ct <- table(data$target, predict(my.lda)$class))
# total error (in percentage)
(lda.tr.error <- 100*(1-sum(diag(prop.table(ct)))))
my.qda <- qda(target ~ X1 + X2, data = data)
(ct <- table(data$target, predict(my.qda)$class))
# total error (in percentage)
(qda.tr.error <- 100*(1-sum(diag(prop.table(ct)))))
library(TunePareto) # for generateCVRuns()
k <- 10
CV.folds <- generateCVRuns(data$target, ntimes=1, nfold=k, stratified=TRUE)
cv.results <- matrix (rep(0,4*k),nrow=k)
colnames (cv.results) <- c("k","fold","TR error","VA error")
cv.results[,"TR error"] <- 0
cv.results[,"VA error"] <- 0
cv.results[,"k"] <- k
priors <- c(prior.1,prior.2) # for clarity
for (j in 1:k)
{
# get VA data
va <- unlist(CV.folds[[1]][[j]])
# train on TR data
my.lda.TR <- lda(target ~ X1 + X2, data = data[-va,], prior=priors, CV=FALSE)
# predict TR data
pred.va <- predict (my.lda.TR)$class
tab <- table(data[-va,]$target, pred.va)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
# predict VA data
pred.va <- predict (my.lda.TR, newdata=data[va,])$class
tab <- table(data[va,]$target, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
## have a look at the results ...
cv.results
k
for (j in 1:k)
{
# get VA data
va <- unlist(CV.folds[[1]][[j]])
# train on TR data
my.lda.TR <- lda(target ~ X1 + X2, data = data[-va,], prior=priors, CV=FALSE)
# predict TR data
pred.va <- predict (my.lda.TR)$class
tab <- table(data[-va,]$target, pred.va)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
# predict VA data
pred.va <- predict (my.lda.TR, newdata=data[va,])$class
tab <- table(data[va,]$target, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
CV.folds <- generateCVRuns(data$target, ntimes=1, nfold=k, stratified=TRUE)
library(TunePareto) # for generateCVRuns()
install.packages("TunePareto")
CV.folds <- generateCVRuns(data$target, ntimes=1, nfold=k, stratified=TRUE)
library(TunePareto) # for generateCVRuns()
for (j in 1:k)
{
# get VA data
va <- unlist(CV.folds[[1]][[j]])
# train on TR data
my.lda.TR <- lda(target ~ X1 + X2, data = data[-va,], prior=priors, CV=FALSE)
# predict TR data
pred.va <- predict (my.lda.TR)$class
tab <- table(data[-va,]$target, pred.va)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
# predict VA data
pred.va <- predict (my.lda.TR, newdata=data[va,])$class
tab <- table(data[va,]$target, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
k <- 10
CV.folds <- generateCVRuns(data$target, ntimes=1, nfold=k, stratified=TRUE)
## prepare the structure to store the partial results
cv.results <- matrix (rep(0,4*k),nrow=k)
colnames (cv.results) <- c("k","fold","TR error","VA error")
cv.results[,"TR error"] <- 0
cv.results[,"VA error"] <- 0
cv.results[,"k"] <- k
## let us first compute the 10-fold CV errors
priors <- c(prior.1,prior.2) # for clarity
for (j in 1:k)
{
# get VA data
va <- unlist(CV.folds[[1]][[j]])
# train on TR data
my.lda.TR <- lda(target ~ X1 + X2, data = data[-va,], prior=priors, CV=FALSE)
# predict TR data
pred.va <- predict (my.lda.TR)$class
tab <- table(data[-va,]$target, pred.va)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
# predict VA data
pred.va <- predict (my.lda.TR, newdata=data[va,])$class
tab <- table(data[va,]$target, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
## have a look at the results ...
cv.results
## What one really uses is the average of the last column
(VA.error <- mean(cv.results[,"VA error"]))
DA.CV <- function (k, method)
{
CV.folds <- generateCVRuns(data$target, ntimes=1, nfold=k, stratified=TRUE)
cv.results <- matrix (rep(0,4*k),nrow=k)
colnames (cv.results) <- c("k","fold","TR error","VA error")
cv.results[,"TR error"] <- 0
cv.results[,"VA error"] <- 0
cv.results[,"k"] <- k
priors <- c(prior.1,prior.2) # for clarity
for (j in 1:k)
{
# get VA data
va <- unlist(CV.folds[[1]][[j]])
# train on TR data
if (method == "LDA") { my.da.TR <- lda(target ~ X1 + X2, data = data[-va,], prior=priors, CV=FALSE) }
else if (method == "QDA") { my.da.TR <- qda(target ~ X1 + X2, data = data[-va,], prior=priors, CV=FALSE) }
else stop("Wrong method")
# predict TR data
pred.va <- predict (my.da.TR)$class
tab <- table(data[-va,]$target, pred.va)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
# predict VA data
pred.va <- predict (my.da.TR, newdata=data[va,])$class
tab <- table(data[va,]$target, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
mean(cv.results[,"VA error"])
}
the.Ks <- 2:20
res <- vector("numeric",length(the.Ks)+1)
res[1] <- NA
for (k in the.Ks) res[k] <- DA.CV(k,"LDA")
plot(res,type="b",xlab="Value of k",ylab="average CV error", ylim=c(0.22,0.3), xaxt="n")
axis(1, at=1:20,labels=1:20, las=2)
grid()
prob.error <- function (Pw1, Pw2, Sigma, Mu1, Mu2)
{
# Numerically correct way for t(x) %*% solve(M) %*% (x), i.e., for x^T M^{-1} x
quad.form.inv <- function (M, x)
{
drop(crossprod(x, solve(M, x)))
}
stopifnot (Pw2+Pw1==1,Pw2>0,Pw1>0,Pw2<1,Pw1<1)
alpha <- log(Pw2/Pw1)
D <- quad.form.inv (Sigma, Mu1-Mu2)
A1 <- (alpha-D/2)/sqrt(D)
A2 <- (alpha+D/2)/sqrt(D)
Pw1*pnorm(A1)+Pw2*(1-pnorm(A2))
}
## In this case we get:
(pe <- prob.error (prior.1,prior.2,R,c(-1,0),c(1,1/2)))
## add it to the plot in blue (it may be that if falls outside the plot, in the bottom)
abline(pe,0, col='blue')
iters <- 20
res <- vector("numeric",length(the.Ks)+1)
res[1] <- NA
for (k in the.Ks) res[k] <- mean(replicate(iters,DA.CV(k,"LDA")))
## let us see the results
plot(res,type="b",xlab="Value of k", ylim=c(0.2,0.3), xaxt="n")
axis(1, at=1:20,labels=1:20, las=2)
# in blue the true error
abline(pe,0,col="blue")
# in red the training error
abline(lda.tr.error/100, 0, col="red")
# in green the LOOCV error (notice this one cannot be iterated)
lda.LOOCV <- lda(target ~ X1 + X2, data = data, prior=priors, CV=TRUE)
(ct <- table(data$target, lda.LOOCV$class))
(lda.LOOCV.error <- 1-sum(diag(prop.table(ct))))
abline(lda.LOOCV.error, 0, col="green")
legend("topright", legend=c("average k-CV error", "true error", "training error", "LOOCV error"),
pch=c(1,1), col=c("black", "blue","red","green"))
(lda.10x10.CV <- mean(replicate(10,DA.CV(10,"LDA"))))
(qda.10x10.CV <- mean(replicate(10,DA.CV(10,"QDA"))))
N.test <- 10000
N1 <- round(prior.1*N.test)
N2 <- N.test - N1
data1 <- mvrnorm(N1, mu=c(-1,0), Sigma=R)
data2 <- mvrnorm(N2, mu=c(1,1/2), Sigma=R)
data.test <- data.frame (rbind(data1,data2),target=as.factor(c(rep('1',N1),rep('2',N2))))
my.lda <- lda(target ~ X1 + X2, data = data)
(ct <- table(data.test$target, predict(my.lda, newdata=data.test)$class))
## total error (in percentage)
(pe.hat <- 1-sum(diag(prop.table(ct))))
dev <- sqrt(pe.hat*(1-pe.hat)/N.test)*1.967
sprintf("(%f,%f)", pe.hat-dev,pe.hat+dev)
## true error of our model was
pe
source("Cross_validation.R")
source("01_datapreparation_david.R")
dataset_raw_continous_vars <- raw_continuous_vars_selection(data)
summary(dataset_raw_continous_vars)
rm(list = ls())
gc()
#### 1 - Set up environment ####
a = Sys.info()
david_wd <- "~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src"
asaf_wd <- ""
pau_wd <- ""
if (a["user"] == "david") {
wd <- david_wd
} else if (a["user"] == "pau") {
wd <- pau_wd
} else {
wd <- asaf_wd
}
setwd(wd)
# source scripts
source("aux_functions.R")
source_scripts()
load_packages()
#### 2 - Read data ####
if (a[1] == "Linux"){
# Linux reading file
data <- read.csv(file="../Dataset/kc_house_data.csv", header=TRUE, sep=",")
} else {
# windows reading file
data <- read.csv(file="../Dataset/kc_house_data.csv", header=TRUE, sep=",")
}
dataset_raw_continous_vars <- raw_continuous_vars_selection(data)
summary(dataset_raw_continous_vars)
linear_model_fitting_original_data(dataset_raw_continous_vars)
dataset_raw_continous_vars <- raw_continuous_vars_selection(data)
linear_model_fitting_original_data(dataset_raw_continous_vars)
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/linear_regressions_fitting.R')
linear_model_fitting_original_data(dataset_raw_continous_vars)
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
linear_regreesion.10x10.CV <- mean(replicate(10,MODEL.CV.OVER.Ks(data,10,"linear_regression")))
res
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
for (k in the.Ks) res[k] <- MODEL.CV(data, k)
summary(data)
method
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
linear_model_fitting_original_data(dataset_raw_continous_vars)
linear_regreesion.10x10.CV <- mean(replicate(10,MODEL.CV.OVER.Ks(data,10,"linear_regression")))
method
linear_model_fitting_original_data(dataset_raw_continous_vars)
linear_model_fitting_original_data(dataset_raw_continous_vars)
dataset_raw_continous_vars <- raw_continuous_vars_selection(data)
names(dataset_raw_continous_vars)
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/01_datapreparation_david.R')
dataset_raw_continous_vars <- raw_continuous_vars_selection(data)
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/01_datapreparation_david.R')
dataset_raw_continous_vars <- raw_continuous_vars_selection(data)
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/01_datapreparation_david.R')
dataset_raw_continous_vars <- raw_continuous_vars_selection(data)
names(data)
names(data)[1]
names(data)["proce"]
names(data)["prrce"]
names(data)["price"]
names(data)[1]
names(data)[1] <- "target"
names(data)
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/01_datapreparation_david.R')
dataset_raw_continous_vars <- raw_continuous_vars_selection(data)
linear_model_fitting_original_data(dataset_raw_continous_vars)
str(CV.folds)
k
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
cv.results
summary(va)
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
lm_model <- lm(price ~ ., data=data)
my_lr_tr <- lm(target ~ ., data=data)
my_lr_tr <- lm(target ~ ., data=data)
predictions <- predict(my_lr_tr, data = data[-va,])
summary(predictions)
head(predictions)
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
linear_model_fitting_original_data(dataset_raw_continous_vars)
va
str(cv.results[j,"fold"] <- j)
str(va)
summary(my_lr_tr)
tr.error
str(tr.pred)
head(tr.pred)
tr.error <- sqrt(sum((tr.pred - data[-va,]$target)^2)/nrow(data[-va,]))
length(tr.pred)
length(tr.pred)
length(data)
nrow(data)
str(CV.folds)
nrow(data)
length(data)
length(va)
length(tr.pred)
length(data[-va])
length( data[-va,]$target)
nrow(data[-va,])
# compute training error NRMSE
tr.pred <- predict(my_lr_tr, data = data[-va,])
length(tr.pred)
# compute training error NRMSE
tr.pred <- predict(my_lr_tr, data = data[-va,])
length(tr.pred)
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
my_lr_tr <- lm(target ~ ., data=data[-va])
my_lr_tr <- lm(target ~ ., data=data[-va,])
my_lr_tr <- lm(target ~ ., data=data[-va,])
# compute training error NRMSE
tr.pred <- predict(my_lr_tr, data = data[-va,])
length(tr.pred)
length(data[-va,]$target)
tr.error <- sqrt(sum((tr.pred - data[-va,]$target)^2)/nrow(data[-va,]))
tr.error
mean(data$target)
# compute testing error NRMSE
va.pred <- predict(my_lr_tr, data = data[va,])
va.error <- sqrt(sum((va.pred - data[va,]$target)^2)/nrow(data[va,]))
length(va.pred)
nrow(data[va,])
?predict
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/linear_regressions_fitting.R')
?predict
# compute testing error NRMSE
va.pred <- predict(my_lr_tr, newdata  = data[va,])
length(va.pred)
va.error <- sqrt(sum((va.pred - data[va,]$target)^2)/nrow(data[va,]))
va.error
cv.results[j,"VA error"] <- va.error
cv.results[j,"fold"] <- j
cv.results
cv.results[j,"TR error"] <- tr.error
cv.results
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
linear_regreesion.10x10.CV <- mean(replicate(10,MODEL.CV.OVER.Ks(data,10,"linear_regression")))
res
matrix(0, nrow = 2, ncol = length(the.Ks)+1)
matrix(0, nrow = length(the.Ks)+1, ncol = 2)
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
for (k in the.Ks) res[k,] <- MODEL.CV(data, kflod, method)
res
res <- matrix(0, nrow = length(the.Ks)+1, ncol = 2)
res[1,] <- NA
res
k
the.Ks <- 2:kflod
the.Ks
k
for (k in the.Ks) res[k,] <- MODEL.CV(data, kflod, method)
the.Ks
res
MODEL.CV(data, kflod, method)
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
linear_model_fitting_original_data(dataset_raw_continous_vars)
res
MODEL.CV(data, kflod, method)
mean(cv.results[,c("TR error","VA error")])
cv.results
apply(cv.results[,c("TR error", "VA error")], 2, mean)
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
dataset_raw_continous_vars <- raw_continuous_vars_selection(data)
linear_model_fitting_original_data(dataset_raw_continous_vars)
res
names(res) <- c("TR erro", "VA error")
res
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
linear_model_fitting_original_data(dataset_raw_continous_vars)
?cat
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
source('~/MIRI-Data Science/ML - Machine Learning/Project/mlproject/src/Cross_validation.R')
linear_model_fitting_original_data(dataset_raw_continous_vars)
linear_regreesion.10x10.CV
